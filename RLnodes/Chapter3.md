# 第三章 表格型方法
## 3.1 马尔可夫决策过程
马尔可夫决策过程的四元组：$<S,A,P,R>$
下面用熊的例子举例。
### 3.1.1 有模型和无模型
有模型：环境的状态转移概率和奖励函数是已知的。
![](https://datawhalechina.github.io/easy-rl/img/ch3/3.2.png)
无模型：环境的状态转移概率和奖励函数是未知的。
![](https://datawhalechina.github.io/easy-rl/img/ch3/3.3.png)
**二者的区别**
有模型的强化学习算法可以直接使用状态转移概率和奖励函数，而无模型的强化学习算法需要通过与环境的交互来估计状态转移概率和奖励函数。

## 3.2 Q表格
在多次尝试和熊打交道之后，我们就可以对熊的不同的状态做出判断，用状态动作价值来表达在某个状态下某个动作的好坏。下面是Q表格：
![](https://datawhalechina.github.io/easy-rl/img/ch3/3.4.png)
**强化**是指智能体在环境中采取某个动作后，环境会给智能体一个奖励，这个奖励会使智能体的状态动作价值发生变化。

## 3.3 免模型预测
回顾预测的定义：根据马尔可夫决策过程，估计状态的价值函数
在无法获取马尔可夫决策过程的模型情况下，我们可以通过蒙特卡洛方法和时序差分方法来估计某个给定策略的价值。
### 3.3.1 蒙特卡洛策略评估
1. 蒙特卡洛方法
**基于采样的方法**：给定策略$\pi$，让智能体和环境交互，得到很多轨迹，每个轨迹对应一个回报：
$$G_t=r_{t+1}+\gamma r_{t+2}+\dots$$
然后对所有的轨迹进行平均，就可以得到某个状态的价值函数：
$$V_\pi(s)=\mathbb{E}_\pi[G_t|s_t=s]$$
蒙特卡洛方法有一定的局限性，它只能用在有终止的马尔可夫决策过程中。  
2. **动态规划**也是常用的估计价值函数的方法。
动态规划方法使用贝尔曼期望备份，通过上一时刻的状态价值函数来更新当前时刻的状态价值函数，直到收敛。
$$V_i(s)\leftarrow\sum_{a\in A}\pi(a\mid s)\left(R(s,a)+\gamma\sum_{s'\in S}P\left(s'\mid s,a\right)V_{i-1}\left(s'\right)\right)$$
动态规划是有模型的方法。与蒙特卡洛方法相比，动态规划方法的优点是它的估计是稳定的，只需要少量的采样就可以得到准确的结果。但是动态规划方法的缺点是它需要环境的所有信息，包括状态转移概率和奖励函数，这些信息在很多情况下是未知的。状态数量很多的时候，计算量会非常大。
### 3.3.2 时序差分
巴甫洛夫的狗的例子。
给狗喂食，狗会分泌唾液。如果不给狗喂食，狗也会分泌唾液。这是因为狗已经学会了，每次听到铃声就会有食物，所以狗会分泌唾液。这个过程就是条件反射。食物可以认为是奖励，铃声可以认为是状态，开始时铃声是中性的，但是经过多次的训练，铃声就会变成有奖励的状态。当铃声这种条件反射巩固之后，再用一种新的刺激，比如闪光灯，每次闪光灯，然后摇铃，狗也会分泌唾液，逐渐形成第二级的条件反射。
**下一个状态的价值会影响当前状态的价值**
个人理解：*在时序差分的迭代过程中，首先会发现有奖励的状态，然后去影响有奖励的状态的前一个状态，然后会影响有奖励的状态的前两个状态，以此类推，直到所有的状态都被影响，最终收敛。*  

时序差分是介于蒙特卡洛方法和动态规划方法之间的一种方法。它是免模型的，不需要转移矩阵和奖励函数。
时序差分方法的目的是对于某个给定的策略$\pi$，在线地算出它的价值函数$V\pi$。最简单的算法是一步时序差分算法（one-step TD algorithm），它的更新公式如下：
$$V(s_t)\leftarrow V(s_t)+\alpha\left[r_{t+1}+\gamma V\left(s^{\prime}\right)-V(s_t)\right]$$
**时序差分目标**：$r_{t+1}+\gamma V(s_{t+1})$，带衰减的未来奖励的总和，由两部分组成：
1. $r_{t+1}$：即时奖励
2. $\gamma V(s_{t+1})$：未来奖励的总和，$\gamma$是折扣因子，$V(s_{t+1})$是下一个状态的价值函数，$\gamma V(s_{t+1})$是下一个状态的价值函数乘以折扣因子，折扣因子的作用是使得未来奖励的总和越来越小，因为未来的奖励越远，我们越不关心，所以折扣因子越小，未来奖励的总和就越小。
时序差分方法和蒙特卡洛方法的比较：
1. 二者都是免模型的价值估计方法，不需要环境的任何信息，只需要与环境进行交互，就可以估计价值函数。
不同点：
1. 时序差分可以在线学习，而蒙特卡洛方法需要等到一个回合结束之后才能更新价值函数。
2. 时序差分可以从不完整的序列中学习，而蒙特卡洛方法只能从完整的序列中学习。
3. 时序差分可以在连续无终止的任务中学习，而蒙特卡洛方法只能在有终止的任务中学习。
4. 时序差分利用了马尔可夫性质，而蒙特卡洛方法没有利用马尔可夫性质。
**蒙特卡洛没有使用自举，动态规划没有使用采样，时序差分方法同时使用了自举和采样。**
![](https://datawhalechina.github.io/easy-rl/img/ch3/comparison_5.png)
## 3.4 免模型控制
**在不知道马尔可夫决策模型的情况下，得到最佳的策略**
回忆策略迭代：
1. 策略评估：根据策略函数，得到状态价值函数
策略评估的方法：蒙特卡洛方法和动态规划方法（迭代）
2. 策略改进
根据状态价值函数求Q函数，然后迭代策略  

对策略迭代进行广义的推广：：**广义策略迭代**  
问题在于：不知道奖励函数和状态转移概率。之前是通过贝尔曼期望方程来求Q函数，现在不知道奖励函数和状态转移概率，所以无法求Q函数。解决办法是用蒙特卡洛方法估计Q函数。
![](https://datawhalechina.github.io/easy-rl/img/ch3/model_free_control_3.png)
使用蒙特卡洛方法替代动态规划方法估计Q函数，然后进行策略更新：
$$\pi(s)=arg\max_{a}Q(s,a)$$
保证策略迭代收敛的假设是回合有探索性开始。探索性开始保证所有的状态和动作都在无限步的执行后能被采样到，这样才能很好地进行估计。算法如下：
![](https://datawhalechina.github.io/easy-rl/img/ch3/model_free_control_4.png)
**保证蒙特卡洛方法有探索性开始的方法**：***$\epsilon$-贪心策略***
$\epsilon$-贪心策略的定义：以$1-\epsilon$的概率选择最优动作，以$\epsilon$的概率随机选择动作。$\epsilon$通常是一个很小的数，比如0.1，并且随着时间的推移，$\epsilon$会逐渐减小。
价值函数是单调的、改进的，证明如下：
$$\begin{aligned}
Q_\pi(s,\pi^\prime(s)) \\
&= \sum_{a\in A}{\pi^\prime(a|s)Q_\pi(s,a)} \\
&= \frac{\epsilon}{|A|}\sum_{a\in A}{Q_\pi(s,a)}+(1-\epsilon)\max_{a\in A}{Q_\pi(s,a)} \\
&\geq \frac{\epsilon}{|A|}\sum_{a\in A}{Q_\pi(s,a)}+(1-\epsilon)\sum_{a\in A}{\frac{\pi(a|s)-\frac{\epsilon}{|A|}}{1-\epsilon}Q_\pi(s,a)} \\
&= \sum_{a\in A}{\pi(a|s)Q_\pi(s,a)} \\
&= V_\pi(s)
\end{aligned}$$
基于$\epsilon$-贪心策略的蒙特卡洛控制算法：
![](https://datawhalechina.github.io/easy-rl/img/ch3/model_free_control_7.png)
*之前提高过蒙特卡洛方法必需是有终止的马尔可夫决策过程，必需要一个完整的回合才能更新，如果使用时序差分方法，就可以不需要完整的回合，可以在线更新。*
### 3.4.1 Sarsa算法：同策略时序差分控制
利用时序差分方法来估计Q函数
$$Q(s_t,a_t)\leftarrow Q(s_t,a_t)+\alpha\left[r_{t+1}+\gamma Q\left(s_{t+1},a_{t+1}\right)-Q(s_t,a_t)\right]$$
![](https://datawhalechina.github.io/easy-rl/img/ch3/3.14.png)
其中$\alpha$类似于学习率，它的作用是控制更新的幅度，$\alpha$越大，更新的幅度越大，$\alpha$越小，更新的幅度越小。Sarsa属于单步更新算法，每执行一个动作，就会更新一次价值和策略。当然也可以采取n步更新算法，每执行n个动作，就会更新一次价值和策略，得到n步Sarsa算法：
![](https://datawhalechina.github.io/easy-rl/img/ch3/3.15.png)
### 3.4.2 Q学习：异策略时序差分控制
异策略算法：在学习过程中，有两种不同的策略：目标策略(target policy)和行为策略(behavior policy)，目标策略是要学习的策略，行为策略是用来采样的策略。异策略算法的目标是学习目标策略的价值函数，但是采样的策略是行为策略。
在异策略学习的过程中，轨迹都是行为策略与环境交互产生的，但是更新的是目标策略的价值函数。
目标策略：
$$\pi(s_{t+1})=arg\max_{a^\prime}Q(s_{t+1},a^\prime)$$
行为策略采用$\epsilon$-贪心策略，更新公式如下：
$$Q(s_t,a_t)\leftarrow Q(s_t,a_t)+\alpha\left[r_{t+1}+\gamma \max_{a^\prime}Q\left(s_{t+1},a^\prime\right)-Q(s_t,a_t)\right]$$